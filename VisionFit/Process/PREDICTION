{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"L4","machine_shape":"hm","mount_file_id":"1kj_fwTQy5PFMjfj_vdB4Lp1KnEfOlv5X","authorship_tag":"ABX9TyMbFeXf+9UbM2co/IYyFIoI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["**Import Libraries**"],"metadata":{"id":"LMiGKgJTr8CP"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qY_H_pzCrQLx","executionInfo":{"status":"ok","timestamp":1767150829957,"user_tz":-480,"elapsed":10133,"user":{"displayName":"Rafiq Azizi","userId":"06142777677022208421"}},"outputId":"a094d777-df9f-4db0-a189-63a4a269b605"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting mtcnn\n","  Downloading mtcnn-1.0.0-py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: joblib>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from mtcnn) (1.5.3)\n","Collecting lz4>=4.3.3 (from mtcnn)\n","  Downloading lz4-4.4.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n","Downloading mtcnn-1.0.0-py3-none-any.whl (1.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lz4-4.4.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: lz4, mtcnn\n","Successfully installed lz4-4.4.5 mtcnn-1.0.0\n"]}],"source":["!pip install mtcnn\n","import mtcnn\n","from mtcnn.mtcnn import MTCNN"]},{"cell_type":"code","source":["import os\n","import cv2\n","import tensorflow as tf\n","from tensorflow.keras.models import load_model\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pickle\n","\n","%matplotlib inline"],"metadata":{"id":"-yGcI6V3sEvG","executionInfo":{"status":"ok","timestamp":1767150829979,"user_tz":-480,"elapsed":13,"user":{"displayName":"Rafiq Azizi","userId":"06142777677022208421"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["# **Define Functions**"],"metadata":{"id":"cO9Lk2VrsRT7"}},{"cell_type":"code","source":["def crop_and_resize(image, target_w=224, target_h=224):\n","    '''this function crop & resize images to target size by keeping aspect ratio'''\n","    if image.ndim == 2:\n","        img_h, img_w = image.shape             # for Grayscale will be   img_h, img_w = img.shape\n","    elif image.ndim == 3:\n","        img_h, img_w, channels = image.shape   # for RGB will be   img_h, img_w, channels = img.shape\n","    target_aspect_ratio = target_w/target_h\n","    input_aspect_ratio = img_w/img_h\n","\n","    if input_aspect_ratio > target_aspect_ratio:\n","        resize_w = int(input_aspect_ratio*target_h)\n","        resize_h = target_h\n","        img = cv2.resize(image, (resize_w , resize_h))\n","        crop_left = int((resize_w - target_w)/2)  ## crop left/right equally\n","        crop_right = crop_left + target_w\n","        new_img = img[:, crop_left:crop_right]\n","    if input_aspect_ratio < target_aspect_ratio:\n","        resize_w = target_w\n","        resize_h = int(target_w/input_aspect_ratio)\n","        img = cv2.resize(image, (resize_w , resize_h))\n","        crop_top = int((resize_h - target_h)/4)   ## crop the top by 1/4 and bottom by 3/4 -- can be changed\n","        crop_bottom = crop_top + target_h\n","        new_img = img[crop_top:crop_bottom, :]\n","    if input_aspect_ratio == target_aspect_ratio:\n","        new_img = cv2.resize(image, (target_w, target_h))\n","\n","    return new_img"],"metadata":{"id":"bRO5Xfr7sS4a","executionInfo":{"status":"ok","timestamp":1767150829982,"user_tz":-480,"elapsed":1,"user":{"displayName":"Rafiq Azizi","userId":"06142777677022208421"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["detector = MTCNN()  # creates detector\n","\n","def extract_face(img, target_size=(224,224)):\n","    '''this functions extract the face from different images by\n","    1) finds the facial bounding box\n","    2) slightly expands top & bottom boundaries to include the whole face\n","    3) crop into a square shape\n","    4) resize to target image size for modelling\n","    5) if the facial bounding box in step 1 is not found, image will be cropped & resized to 224x224 square'''\n","\n","    # 1. detect faces in an image\n","\n","    results = detector.detect_faces(img)\n","    if results == []:    # if face is not detected, call function to crop & resize by keeping aspect ratio\n","        new_face = crop_and_resize(img, target_w=224, target_h=224)\n","    else:\n","        x1, y1, width, height = results[0]['box']\n","        x2, y2 = x1+width, y1+height\n","        face = img[y1:y2, x1:x2]  # this is the face image from the bounding box before expanding bbox\n","\n","        # 2. expand the top & bottom of bounding box by 10 pixels to ensure it captures the whole face\n","        adj_h = 10\n","\n","        #assign value of new y1\n","        if y1-adj_h <10:\n","            new_y1=0\n","        else:\n","            new_y1 = y1-adj_h\n","\n","        #assign value of new y2\n","        if y1+height+adj_h < img.shape[0]:\n","            new_y2 = y1+height+adj_h\n","        else:\n","            new_y2 = img.shape[0]\n","        new_height = new_y2 - new_y1\n","\n","        # 3. crop the image to a square image by setting the width = new_height and expand the box to new width\n","        adj_w = int((new_height-width)/2)\n","\n","        #assign value of new x1\n","        if x1-adj_w < 0:\n","            new_x1=0\n","        else:\n","            new_x1 = x1-adj_w\n","\n","        #assign value of new x2\n","        if x2+adj_w > img.shape[1]:\n","            new_x2 = img.shape[1]\n","        else:\n","            new_x2 = x2+adj_w\n","        new_face = img[new_y1:new_y2, new_x1:new_x2]  # face-cropped square image based on original resolution\n","\n","    # 4. resize image to the target pixel size\n","    sqr_img = cv2.resize(new_face, target_size)\n","    return sqr_img"],"metadata":{"id":"psfAfRiKsV8t","executionInfo":{"status":"ok","timestamp":1767150832277,"user_tz":-480,"elapsed":2286,"user":{"displayName":"Rafiq Azizi","userId":"06142777677022208421"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["y_label_dict = {0: 'Heart', 1: 'Oblong', 2: 'Oval', 3: 'Round', 4: 'Square'}\n","\n","def predict_face_shape(img_array):\n","    '''\n","    this function reads a single image in the form of an array,\n","    and process the image then make predictions.\n","    '''\n","    try:\n","        # first extract the face using bounding box\n","        face_img = extract_face(img_array)  # call function to extract face with bounding box\n","        new_img = cv2.cvtColor(face_img,cv2.COLOR_BGR2RGB) # convert to RGB -- use this for display\n","        # convert the image for modelling\n","        test_img = np.array(new_img, dtype=float)\n","        test_img = test_img/255\n","        test_img = np.array(test_img).reshape(1, 224, 224, 3)\n","        # make predictions\n","        pred = model.predict(test_img)\n","        label = np.argmax(pred,axis=1)\n","        shape = y_label_dict[label[0]]\n","        print(f'Your face shape is {shape}')\n","        pred = np.max(pred)\n","        print(f'Probability {np.around(pred*100,2)}')\n","        plt.imshow(new_img)\n","    except Exception as e:\n","        print(f'Oops!  Something went wrong.  Please try again.')"],"metadata":{"id":"SU81_whssZ7F","executionInfo":{"status":"ok","timestamp":1767150832313,"user_tz":-480,"elapsed":19,"user":{"displayName":"Rafiq Azizi","userId":"06142777677022208421"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["# **Load saved models for predictions**"],"metadata":{"id":"YOe6dluisqE8"}},{"cell_type":"markdown","source":[],"metadata":{"id":"5wpeRnM8sqDu"}},{"cell_type":"code","source":["# Transfer Learning model from VGG-Face\n","\n","transfer_path = '/content/drive/MyDrive/Google Collab/Face_Shape_Classification_saved_models/model_t8.keras'\n","transfer_file = transfer_path\n","model = tf.keras.models.load_model(transfer_file)"],"metadata":{"id":"x-68aytfsbsB","executionInfo":{"status":"ok","timestamp":1767150837556,"user_tz":-480,"elapsed":5240,"user":{"displayName":"Rafiq Azizi","userId":"06142777677022208421"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["# **Load some images for testing**"],"metadata":{"id":"sTkYFHIls3_1"}},{"cell_type":"code","source":["base_path = '/content/drive/MyDrive/Google Collab/dataset_prediction'\n","\n","import os\n","import cv2\n","\n","image_list = []\n","image_name = []\n","\n","for folder in os.listdir(base_path):\n","    folder_path = os.path.join(base_path, folder)\n","\n","    if os.path.isdir(folder_path):\n","        for file in os.listdir(folder_path):\n","            img_path = os.path.join(folder_path, file)\n","            img = cv2.imread(img_path)\n","\n","            if img is not None:\n","                image_list.append(img)\n","                image_name.append(f\"{folder}/{file}\")\n","\n","len(image_list)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"trPgRsiuR7HK","executionInfo":{"status":"ok","timestamp":1767150845885,"user_tz":-480,"elapsed":8312,"user":{"displayName":"Rafiq Azizi","userId":"06142777677022208421"}},"outputId":"3ea87e89-bc8d-4f8d-fa77-6b874c4cbbae"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["15"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import math\n","\n","results = []\n","\n","# Collect prediction results\n","for img, name in zip(image_list, image_name):\n","\n","    face_img = extract_face(img)\n","    face_rgb = cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB)\n","\n","    test_img = np.array(face_rgb, dtype=float) / 255.0\n","    test_img = test_img.reshape(1,224,224,3)\n","\n","    pred = model.predict(test_img, verbose=0)\n","    label = np.argmax(pred, axis=1)[0]\n","    shape = y_label_dict[label]\n","    confidence = np.max(pred) * 100\n","\n","    results.append((face_rgb, name, shape, confidence))\n","\n","# ---- Display in grid format ----\n","cols = 4   # boleh tukar ke 4 kalau banyak image\n","rows = math.ceil(len(results) / cols)\n","\n","plt.figure(figsize=(cols*5, rows*6))\n","\n","for i, (img, name, shape, conf) in enumerate(results):\n","    plt.subplot(rows, cols, i+1)\n","    plt.imshow(img)\n","    plt.axis('off')\n","\n","    plt.title(\n","        f\"File: {name}\\n\"\n","        f\"Prediction: {shape}\\n\"\n","        f\"Confidence: {conf:.2f}%\",\n","        fontsize=20\n","    )\n","\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1FjG6r95wHLlth8gVvcQCeJT0r0SLpdkr"},"id":"N-vmyfy6TEw1","executionInfo":{"status":"ok","timestamp":1767150859586,"user_tz":-480,"elapsed":13706,"user":{"displayName":"Rafiq Azizi","userId":"06142777677022208421"}},"outputId":"30ddc311-cf99-4be0-c4f9-1315c4fc4019"},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}]}